{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import os\n",
    "from snnUtils import *\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random, imageio, time, copy\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the super parameters\n",
    "projectName = \"snnRL\"\n",
    "\n",
    "# Save/Get weights from presistent storage. Pass empty string for not saving. \n",
    "# Pass derive for using google derive (If code is running in colab). If local, \n",
    "# pass the location of your desire\n",
    "savePath = \"./Data\"\n",
    "backUpNetworks = False \n",
    "saveLen = 30 # Number of latest checkpoints to save\n",
    "\n",
    "# Handle save path\n",
    "if savePath != None:\n",
    "    if savePath == \"derive\":\n",
    "        # Mount gdrive if we want to interact with cloab\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/gdrive')\n",
    "        savePath = \"gdrive/MyDrive/Colab Notebooks/Data/\"\n",
    "    backUpNetworks = True\n",
    "\n",
    "# Making the environment\n",
    "env = gym.make(\"LunarLander-v3\") # Use render_mode = \"human\" to render each episode\n",
    "state, info = env.reset() # Get a sample state of the environment\n",
    "stateSize = env.observation_space.shape # Number of variables to define current step\n",
    "nActions = env.action_space.n # Number of actions\n",
    "nObs = len(state) # Number of features\n",
    "\n",
    "\n",
    "# Set pytorch parameters: The device (CPU or GPU) and data types\n",
    "__device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "__dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Spino.shop\\AppData\\Local\\Temp\\ipykernel_7052\\666578204.py:103: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  qNetworkSaveHistory = torch.load(os.path.join(savePath, _file))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded network weights for episode 1359\n"
     ]
    }
   ],
   "source": [
    "class qNetwork_SNN(nn.Module):\n",
    "    def __init__(self, inputSize, L1Size, L2Size, L3Size, L4Size, outputSize, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # Model super parameters\n",
    "        self.beta = kwargs[\"beta\"]\n",
    "        self.tSteps = kwargs[\"tSteps\"]\n",
    "\n",
    "        # Defining the layers\n",
    "        self.layer1 = nn.Linear(inputSize, L1Size)\n",
    "        self.L1LIF = snn.Leaky(beta = self.beta)\n",
    "        self.layer2 = nn.Linear(L1Size, L2Size)\n",
    "        self.L2LIF = snn.Leaky(beta = self.beta)\n",
    "        self.layer3 = nn.Linear(L2Size, L3Size)\n",
    "        self.L3LIF = snn.Leaky(beta = self.beta)\n",
    "        self.layer4 = nn.Linear(L3Size, L4Size)\n",
    "        self.L4LIF = snn.Leaky(beta = self.beta)\n",
    "        self.output = nn.Linear(L4Size, outputSize)\n",
    "        self.outputLIF = snn.Leaky(beta = self.beta)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Set initial potentials to be zero\n",
    "        potential1 = self.L1LIF.reset_mem()\n",
    "        potential2 = self.L2LIF.reset_mem()\n",
    "        potential3 = self.L3LIF.reset_mem()\n",
    "        potential4 = self.L4LIF.reset_mem()\n",
    "        potential5 = self.outputLIF.reset_mem()\n",
    "\n",
    "        # Save the state of the output layer\n",
    "        outSpikes = []\n",
    "        outPotentials = []\n",
    "\n",
    "        # Iterate through time steps\n",
    "        for t in range(self.tSteps):\n",
    "            # First layer\n",
    "            current1 = self.layer1(x)\n",
    "            spk1, potential1 = self.L1LIF(current1, potential1)\n",
    "\n",
    "            # Second layer\n",
    "            current2 = self.layer2(spk1)\n",
    "            spk2, potential2 = self.L2LIF(current2, potential2)\n",
    "\n",
    "            # Third layer\n",
    "            current3 = self.layer3(spk2)\n",
    "            spk3, potential3 = self.L3LIF(current3, potential3)\n",
    "\n",
    "            # Fourth layer\n",
    "            current4 = self.layer4(spk3)\n",
    "            spk4, potential4 = self.L4LIF(current4, potential4)\n",
    "\n",
    "            #Output\n",
    "            current5 = self.output(spk4)\n",
    "            spk5, potential5 = self.outputLIF(current5, potential5)\n",
    "\n",
    "            # Save output\n",
    "            outSpikes.append(spk5)\n",
    "            outPotentials.append(potential5)\n",
    "\n",
    "        return torch.stack(outSpikes, dim = 0).sum(dim = 0)\n",
    "\n",
    "# Model parameters\n",
    "nL1, nL2, nL3, nL4 = 128, 64, 32, 16\n",
    "learningRate = .00001\n",
    "timeSteps = 150\n",
    "snnBeta = .95\n",
    "eDecay = 0.999\n",
    "miniBatchSize = 1000 # The length of minibatch that is used for training\n",
    "gamma = .995 # The discount factor\n",
    "extraInfo = \"\"\n",
    "modelDetails = f\"{nL1}_{nL2}_{nL3}_{nL4}_{learningRate}_{timeSteps}_{snnBeta}_{eDecay}_{miniBatchSize}_{gamma}_{extraInfo}\"\n",
    "\n",
    "# Make the model objects\n",
    "qNetwork_model = qNetwork_SNN(stateSize[0], nL1, nL2, nL3, nL4, nActions, beta = snnBeta, tSteps = timeSteps).to(__device, dtype = __dtype)\n",
    "targetQNetwork_model = qNetwork_SNN(stateSize[0], nL1, nL2, nL3, nL4, nActions, beta = snnBeta, tSteps = timeSteps).to(__device, dtype = __dtype)\n",
    "\n",
    "# Two models should have identical weights initially\n",
    "targetQNetwork_model.load_state_dict(qNetwork_model.state_dict())\n",
    "\n",
    "# TODO: Add gradient clipping to the optimizer for avoiding exploding gradients\n",
    "# Suitable optimizer for gradient descent\n",
    "optimizer_main = torch.optim.Adam(qNetwork_model.parameters(), lr=learningRate)\n",
    "optimizer_target = torch.optim.Adam(targetQNetwork_model.parameters(), lr=learningRate)\n",
    "\n",
    "# Starting episode and ebsilon\n",
    "startEpisode = 0\n",
    "startEbsilon = None\n",
    "lstHistory = None\n",
    "\n",
    "# Making the memory buffer object\n",
    "memorySize = 100_000 # The length of the entire memory\n",
    "mem = ReplayMemory(memorySize, __dtype, __device)\n",
    "\n",
    "# If given access to drive, try to load the latest saved weights\n",
    "qNetworkSaveHistory = deque(maxlen = saveLen)\n",
    "targetQNetworkSaveHistory = deque(maxlen = saveLen)\n",
    "if backUpNetworks:\n",
    "    if os.path.isdir(savePath):\n",
    "        _lst = os.listdir(savePath)\n",
    "        for _file in _lst:\n",
    "            if f\"{projectName}_{modelDetails}.pth\" == _file:\n",
    "                qNetworkSaveHistory = torch.load(os.path.join(savePath, _file))\n",
    "                qNetworkSaveHistory = qNetworkSaveHistory if isinstance(qNetworkSaveHistory, list) else [qNetworkSaveHistory]\n",
    "                _chekcPoint = qNetworkSaveHistory[0] # Take the most recent chekcpoint\n",
    "\n",
    "                # Load Q-Network\n",
    "                qNetwork_model.load_state_dict(_chekcPoint[\"qNetwork_state_dict\"]) # Model weights\n",
    "                optimizer_main.load_state_dict(_chekcPoint[\"qNetwork_optimizer_state_dict\"]) # Optimizer\n",
    "\n",
    "                # Load target Q-Network\n",
    "                targetQNetwork_model.load_state_dict(_chekcPoint[\"targetQNetwork_state_dict\"]) # Model weights\n",
    "                \n",
    "                # Load process parameters\n",
    "                startEpisode = int(_chekcPoint[\"episode\"]) # Starting episode number\n",
    "                startEbsilon = float(_chekcPoint[\"hyperparameters\"][\"ebsilon\"]) # Starting ebsilon\n",
    "                lstHistory = _chekcPoint[\"train_history\"]\n",
    "                eDecay = _chekcPoint[\"hyperparameters\"][\"eDecay\"] if \"eDecay\" in _chekcPoint[\"hyperparameters\"].keys() else None\n",
    "\n",
    "                if \"experiences\" in _chekcPoint.keys():\n",
    "                    mem.loadExperiences(\n",
    "                        _chekcPoint[\"experiences\"][\"state\"],\n",
    "                        _chekcPoint[\"experiences\"][\"action\"],\n",
    "                        _chekcPoint[\"experiences\"][\"reward\"],\n",
    "                        _chekcPoint[\"experiences\"][\"nextState\"],\n",
    "                        _chekcPoint[\"experiences\"][\"done\"],\n",
    "                    )\n",
    "\n",
    "                # Backup the current file to avoide data loss in future read/writes (if reading is successful)\n",
    "                import shutil\n",
    "                shutil.copyfile(os.path.join(savePath, _file), os.path.join(savePath, _file.replace(\".pth\", \"_Backup.pth\")))\n",
    "                print(f\"Loaded network weights for episode {startEpisode}\")\n",
    "    else:\n",
    "        print(\"Save path doesn't exist. Making it.\")\n",
    "        os.makedirs(savePath)\n",
    "\n",
    "beginning_qNetwork = [qNetwork_model.layer1.weight, qNetwork_model.layer2.weight, qNetwork_model.output.weight]\n",
    "beginning_targeQNetwork = [targetQNetwork_model.layer1.weight, targetQNetwork_model.layer2.weight, targetQNetwork_model.output.weight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElapsedTime: 2    s | Episode: 1359  | Timestep: 2     | The average of the 100   episodes is: 26   \n",
      "Latest chekpoint: 0 | Speed 0.7 tps | ebsilon: 0.255\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 29>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m     experience \u001b[38;5;241m=\u001b[39m mem\u001b[38;5;241m.\u001b[39msample(miniBatchSize)\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# Update the Q-Network and the target Q-Network\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# Bear in mind that we do not update the target Q-network with direct gradient descent.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# so there is no optimizer needed for it\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[43mfitQNetworks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mqNetwork_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_main\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtargetQNetwork_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Save the necessary data\u001b[39;00m\n\u001b[0;32m     73\u001b[0m points \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[1;32mc:\\Users\\Spino.shop\\Desktop\\Trading\\Apps\\Python_29-_RL_SNN\\snnUtils.py:211\u001b[0m, in \u001b[0;36mfitQNetworks\u001b[1;34m(experience, gamma, qNetwork, target_qNetwork)\u001b[0m\n\u001b[0;32m    208\u001b[0m __targetQNetworkModel \u001b[38;5;241m=\u001b[39m target_qNetwork[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# Update the Q network's weights\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcomputeLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m__qNetworkModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m__targetQNetworkModel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m __qNetworkModel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    214\u001b[0m __targetQNetworkModel\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Users\\Spino.shop\\Desktop\\Trading\\Apps\\Python_29-_RL_SNN\\snnUtils.py:175\u001b[0m, in \u001b[0;36mcomputeLoss\u001b[1;34m(experiences, gamma, qNetwork, target_qNetwork)\u001b[0m\n\u001b[0;32m    171\u001b[0m qNetwork\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# To implement the calculation scheme explained in comments, we multiply Qhat by (1-done).\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# If the episode has terminated done == True so (1-done) = 0.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m Qhat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mamax(\u001b[43mtarget_qNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnextState\u001b[49m\u001b[43m)\u001b[49m, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    176\u001b[0m yTarget \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m  Qhat \u001b[38;5;241m*\u001b[39m ((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m done)) \u001b[38;5;66;03m# Using the bellman equation\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# IMPORTANT: When getting qValues, we have to account for the ebsilon-greedy algorithm as well.\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# This is why we dont use max(qValues in each state) but instead we use the qValues of the taken\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;66;03m# action in that step.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Spino.shop\\Desktop\\Trading\\Apps\\.MotherVenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Spino.shop\\Desktop\\Trading\\Apps\\.MotherVenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36mqNetwork_SNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtSteps):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# First layer\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     current1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m---> 39\u001b[0m     spk1, potential1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mL1LIF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpotential1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Second layer\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     current2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(spk1)\n",
      "File \u001b[1;32mc:\\Users\\Spino.shop\\Desktop\\Trading\\Apps\\.MotherVenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Spino.shop\\Desktop\\Trading\\Apps\\.MotherVenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Spino.shop\\Desktop\\Trading\\Apps\\.MotherVenv\\lib\\site-packages\\snntorch\\_neurons\\leaky.py:220\u001b[0m, in \u001b[0;36mLeaky.forward\u001b[1;34m(self, input_, mem)\u001b[0m\n\u001b[0;32m    216\u001b[0m     spk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfire_inhibition(\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem\n\u001b[0;32m    218\u001b[0m     )  \u001b[38;5;66;03m# batch_size\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 220\u001b[0m     spk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_delay:\n\u001b[0;32m    223\u001b[0m     do_reset \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    224\u001b[0m         spk \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraded_spikes_factor \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset\n\u001b[0;32m    225\u001b[0m     )  \u001b[38;5;66;03m# avoid double reset\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Spino.shop\\Desktop\\Trading\\Apps\\.MotherVenv\\lib\\site-packages\\snntorch\\_neurons\\neurons.py:80\u001b[0m, in \u001b[0;36mSpikingNeuron.fire\u001b[1;34m(self, mem)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_quant:\n\u001b[0;32m     78\u001b[0m     mem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_quant(mem)\n\u001b[1;32m---> 80\u001b[0m mem_shift \u001b[38;5;241m=\u001b[39m \u001b[43mmem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold\u001b[49m\n\u001b[0;32m     81\u001b[0m spk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspike_grad(mem_shift)\n\u001b[0;32m     83\u001b[0m spk \u001b[38;5;241m=\u001b[39m spk \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraded_spikes_factor\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"Device is: {__device}\")\n",
    "\n",
    "# Start the timer\n",
    "tstart = time.time()\n",
    "\n",
    "# The experience of the agent is saved as a named tuple containing various variables\n",
    "agentExp = namedtuple(\"exp\", [\"state\", \"action\", \"reward\", \"nextState\", \"done\"])\n",
    "\n",
    "# Parameters\n",
    "nEpisodes = 6000 # Number of learning episodes\n",
    "maxNumTimeSteps = 1000 # The number of time step in each episode\n",
    "ebsilon = 1 if startEbsilon == None else startEbsilon # The starting  value of ebsilon\n",
    "ebsilonEnd   = .1 # The finishing value of ebsilon\n",
    "eDecay = eDecay # The rate at which ebsilon decays\n",
    "numUpdateTS = 4 # Frequency of time steps to update the NNs\n",
    "numP_Average = 100 # The number of previous episodes for calculating the average episode reward\n",
    "\n",
    "# Variables for saving the required data for later analysis\n",
    "episodePointHist = [] # For saving each episode's point for later demonstration\n",
    "episodeTimeHist = [] # For saving the time it took for episode to end\n",
    "actionString = \"\" # A string containing consecutive actions taken in an episode (dellimited by comma, i.e. 1,2,4,2,1 etc.)\n",
    "episodeHistDf = None\n",
    "lstHistory = [] if lstHistory == None else lstHistory\n",
    "initialCond = None # initial condition (state) of the episode\n",
    "epPointAvg = -999999 if len(lstHistory) == 0 else pd.DataFrame(lstHistory).iloc[-numP_Average:][\"points\"].mean()\n",
    "latestChekpoint = 0\n",
    "\n",
    "\n",
    "for episode in range(startEpisode, nEpisodes):\n",
    "    initialSeed = random.randint(1,1_000_000_000) # The random seed that determines the episode's I.C.\n",
    "    state, info = env.reset(seed = initialSeed)\n",
    "    points = 0\n",
    "    actionString = \"\"\n",
    "    initialCond = state\n",
    "\n",
    "    tempTime = time.time()\n",
    "    _lastPrinttime = tempTime # For printing the training progress \n",
    "    for t in range(maxNumTimeSteps):\n",
    "\n",
    "        qValueForActions = qNetwork_model(torch.tensor(state, device = __device, dtype = __dtype))\n",
    "\n",
    "        # use ebsilon-Greedy algorithm to take the new step\n",
    "        action = getAction(qValueForActions, ebsilon)\n",
    "\n",
    "        # Take a step\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # Store the experience of the current step in an experience deque.\n",
    "        mem.addNew(\n",
    "            agentExp(\n",
    "                state, # Current state\n",
    "                action,\n",
    "                reward, # Current state's reward\n",
    "                observation, # Next state\n",
    "                True if terminated or truncated else False\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Check to see if we have to update the networks in the current step\n",
    "        update = updateNetworks(t, mem, miniBatchSize, numUpdateTS)\n",
    "\n",
    "        if update:\n",
    "            initial_weights = {name: param.clone() for name, param in qNetwork_model.named_parameters()}\n",
    "            # Update the NNs\n",
    "            experience = mem.sample(miniBatchSize)\n",
    "\n",
    "            # Update the Q-Network and the target Q-Network\n",
    "            # Bear in mind that we do not update the target Q-network with direct gradient descent.\n",
    "            # so there is no optimizer needed for it\n",
    "            fitQNetworks(experience, gamma, [qNetwork_model, optimizer_main], [targetQNetwork_model, None])\n",
    "\n",
    "        # Save the necessary data\n",
    "        points += reward\n",
    "        state = observation.copy()\n",
    "        actionString += f\"{action},\"\n",
    "\n",
    "        # Print the training status. Print only once each second to avoid jitters.\n",
    "        if 1 < (time.time() - _lastPrinttime):\n",
    "            clear_output(wait=True)\n",
    "            _lastPrinttime = time.time()\n",
    "            print(f\"ElapsedTime: {int(time.time() - tstart): <5}s | Episode: {episode: <5} | Timestep: {t: <5} | The average of the {numP_Average: <5} episodes is: {int(epPointAvg): <5}\")\n",
    "            print(f\"Latest chekpoint: {latestChekpoint} | Speed {t/(time.time()-tempTime):.1f} tps | ebsilon: {ebsilon:.3f}\")\n",
    "\n",
    "            # fig= plt.figure(figsize=(12,6))\n",
    "            # plt.plot(pd.DataFrame(lstHistory)[\"episode\"], pd.DataFrame(lstHistory)[\"points\"])\n",
    "            # plt.show()\n",
    "\n",
    "        # Handle episode ending\n",
    "        if terminated or truncated:\n",
    "            # Save the episode history in dataframe\n",
    "            if (episode+1) % 3 == 0:\n",
    "                # only save every 10 episodes\n",
    "                lstHistory.append({\n",
    "                    \"episode\": episode,\n",
    "                    \"seed\": initialSeed,\n",
    "                    \"points\": points,\n",
    "                    \"timesteps\": t,\n",
    "                    \"duration\": time.time() - tempTime\n",
    "                })\n",
    "                \n",
    "            break\n",
    "\n",
    "    # Saving the current episode's points and time\n",
    "    episodePointHist.append(points)\n",
    "    episodeTimeHist.append(time.time()-tempTime)\n",
    "\n",
    "    # Getting the average of {numP_Average} episodes\n",
    "    epPointAvg = np.mean(episodePointHist[-numP_Average:])\n",
    "\n",
    "    # Decay ebsilon\n",
    "    ebsilon = decayEbsilon(ebsilon, eDecay, ebsilonEnd)\n",
    "\n",
    "    # Save model weights and parameters periodically (For later use)\n",
    "    if backUpNetworks:\n",
    "        if (episode + 1) % 20 == 0:\n",
    "            _exp = mem.exportExpereince()\n",
    "            _chekcPoint = {\n",
    "                \"episode\": episode,\n",
    "                'qNetwork_state_dict': qNetwork_model.state_dict(),\n",
    "                'qNetwork_optimizer_state_dict': optimizer_main.state_dict(),\n",
    "                'targetQNetwork_state_dict': targetQNetwork_model.state_dict(),\n",
    "                'targetQNetwork_optimizer_state_dict': optimizer_target.state_dict(),\n",
    "                'hyperparameters': {\"ebsilon\": ebsilon, \"eDecay\":eDecay},\n",
    "                \"train_history\": lstHistory,\n",
    "                \"experiences\": {\n",
    "                    \"state\": _exp[\"state\"],\n",
    "                    \"action\": _exp[\"action\"],\n",
    "                    \"reward\": _exp[\"reward\"],\n",
    "                    \"nextState\": _exp[\"nextState\"],\n",
    "                    \"done\": _exp[\"done\"]\n",
    "                }\n",
    "            }\n",
    "            qNetworkSaveHistory.appendleft(_chekcPoint)\n",
    "            torch.save(qNetworkSaveHistory, os.path.join(savePath, f\"{projectName}_{modelDetails}.pth\"))\n",
    "\n",
    "            # Save the episode number\n",
    "            latestChekpoint = episode\n",
    "\n",
    "    # Stop the learning process if suitable average point is reacheds\n",
    "    if 100 < epPointAvg:\n",
    "        Tend = time.time()\n",
    "        print(f\"\\nThe learning ended. Elapsed time for learning: {Tend-tstart:.2f}s. \\nAVG of latest 100 episodes: {epPointAvg}\")\n",
    "        \n",
    "        _exp = mem.exportExpereince()\n",
    "        _chekcPoint = {\n",
    "            \"episode\": episode,\n",
    "            'qNetwork_state_dict': qNetwork_model.state_dict(),\n",
    "            'qNetwork_optimizer_state_dict': optimizer_main.state_dict(),\n",
    "            'targetQNetwork_state_dict': targetQNetwork_model.state_dict(),\n",
    "            'targetQNetwork_optimizer_state_dict': optimizer_target.state_dict(),\n",
    "            'hyperparameters': {\"ebsilon\": ebsilon, \"eDecay\":eDecay},\n",
    "            \"train_history\": lstHistory,\n",
    "            \"experiences\": {\n",
    "                \"state\": _exp[\"state\"],\n",
    "                \"action\": _exp[\"action\"],\n",
    "                \"reward\": _exp[\"reward\"],\n",
    "                \"nextState\": _exp[\"nextState\"],\n",
    "                \"done\": _exp[\"done\"]\n",
    "            }\n",
    "        }\n",
    "        qNetworkSaveHistory.appendleft(_chekcPoint)\n",
    "        torch.save(qNetworkSaveHistory, os.path.join(savePath, f\"{projectName}_{modelDetails}.pth\"))\n",
    "\n",
    "        # Save the episode number\n",
    "        latestChekpoint = episode\n",
    "        \n",
    "        break\n",
    "\n",
    "# Reset the index\n",
    "episodeHistDf = pd.DataFrame(lstHistory)\n",
    "episodeHistDf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig= plt.figure(figsize=(12,6))\n",
    "plt.plot(pd.DataFrame(lstHistory)[\"episode\"], pd.DataFrame(lstHistory)[\"points\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".MotherVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
