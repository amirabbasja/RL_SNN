{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4de84c7",
   "metadata": {},
   "source": [
    "A simple ANN structure to train a DQN using a single agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d8476b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import os\n",
    "from utils import *\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random, imageio, time, copy\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a3cbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the super parameters\n",
    "projectName = \"annDQN\"\n",
    "\n",
    "# Save/Get weights from persistent storage. Pass empty string for not saving. \n",
    "# Pass drive for using google derive (If code is running in colab). If local, \n",
    "# pass the location of your desire\n",
    "savePath = None\n",
    "backUpNetworks = False \n",
    "saveLen = 30 # Number of latest checkpoints to save\n",
    "\n",
    "# Handle save path\n",
    "if savePath != None:\n",
    "    if savePath == \"derive\":\n",
    "        # Mount gdrive if we want to interact with cloab\n",
    "        from google.colab import drive # Only works in google colab\n",
    "        drive.mount('/content/gdrive')\n",
    "        savePath = \"gdrive/MyDrive/Colab Notebooks/Data/\"\n",
    "    backUpNetworks = True\n",
    "\n",
    "# Making the environment\n",
    "env = gym.make(\"LunarLander-v3\") # Use render_mode = \"human\" to render each episode\n",
    "state, info = env.reset() # Get a sample state of the environment\n",
    "stateSize = env.observation_space.shape # Number of variables to define current step\n",
    "nActions = env.action_space.n # Number of actions\n",
    "nObs = len(state) # Number of features\n",
    "\n",
    "\n",
    "# Set pytorch parameters: The device (CPU or GPU) and data types\n",
    "__device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "__dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39eeb715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import qNetwork_ANN\n",
    "\n",
    "# Model parameters\n",
    "nL1, nL2 = 64, 64\n",
    "learningRate = .001\n",
    "eDecay = 0.995\n",
    "miniBatchSize = 1000 # The length of minibatch that is used for training\n",
    "gamma = .995 # The discount factor\n",
    "extraInfo = \"\"\n",
    "modelDetails = f\"{nL1}_{nL2}_{learningRate}_{eDecay}_{miniBatchSize}_{gamma}_{extraInfo}\"\n",
    "\n",
    "# Make the model objects\n",
    "qNetwork_model = qNetwork_ANN([stateSize[0], nL1, nL2, nActions]).to(__device, dtype = __dtype)\n",
    "targetQNetwork_model = qNetwork_ANN([stateSize[0], nL1, nL2, nActions]).to(__device, dtype = __dtype)\n",
    "\n",
    "# Two models should have identical weights initially\n",
    "targetQNetwork_model.load_state_dict(qNetwork_model.state_dict())\n",
    "\n",
    "# TODO: Add gradient clipping to the optimizer for avoiding exploding gradients\n",
    "# Suitable optimizer for gradient descent\n",
    "optimizer_main = torch.optim.Adam(qNetwork_model.parameters(), lr=learningRate)\n",
    "optimizer_target = torch.optim.Adam(targetQNetwork_model.parameters(), lr=learningRate)\n",
    "\n",
    "# Starting episode and ebsilon\n",
    "startEpisode = 0\n",
    "startEbsilon = None\n",
    "lstHistory = None\n",
    "\n",
    "# Making the memory buffer object\n",
    "memorySize = 100_000 # The length of the entire memory\n",
    "mem = ReplayMemory(memorySize, __dtype, __device)\n",
    "\n",
    "# If given access to drive, try to load the latest saved weights\n",
    "qNetworkSaveHistory = deque(maxlen = saveLen)\n",
    "targetQNetworkSaveHistory = deque(maxlen = saveLen)\n",
    "if backUpNetworks:\n",
    "    if os.path.isdir(savePath):\n",
    "        _lst = os.listdir(savePath)\n",
    "        for _file in _lst:\n",
    "            if f\"{projectName}_{modelDetails}.pth\" == _file:\n",
    "                qNetworkSaveHistory = torch.load(os.path.join(savePath, _file))\n",
    "                qNetworkSaveHistory = qNetworkSaveHistory if isinstance(qNetworkSaveHistory, list) else [qNetworkSaveHistory]\n",
    "                _chekcPoint = qNetworkSaveHistory[0] # Take the most recent chekcpoint\n",
    "\n",
    "                # Load Q-Network\n",
    "                qNetwork_model.load_state_dict(_chekcPoint[\"qNetwork_state_dict\"]) # Model weights\n",
    "                optimizer_main.load_state_dict(_chekcPoint[\"qNetwork_optimizer_state_dict\"]) # Optimizer\n",
    "\n",
    "                # Load target Q-Network\n",
    "                targetQNetwork_model.load_state_dict(_chekcPoint[\"targetQNetwork_state_dict\"]) # Model weights\n",
    "                \n",
    "                # Load process parameters\n",
    "                startEpisode = int(_chekcPoint[\"episode\"]) # Starting episode number\n",
    "                startEbsilon = float(_chekcPoint[\"hyperparameters\"][\"ebsilon\"]) # Starting ebsilon\n",
    "                lstHistory = _chekcPoint[\"train_history\"]\n",
    "                eDecay = _chekcPoint[\"hyperparameters\"][\"eDecay\"] if \"eDecay\" in _chekcPoint[\"hyperparameters\"].keys() else None\n",
    "\n",
    "                if \"experiences\" in _chekcPoint.keys():\n",
    "                    mem.loadExperiences(\n",
    "                        _chekcPoint[\"experiences\"][\"state\"],\n",
    "                        _chekcPoint[\"experiences\"][\"action\"],\n",
    "                        _chekcPoint[\"experiences\"][\"reward\"],\n",
    "                        _chekcPoint[\"experiences\"][\"nextState\"],\n",
    "                        _chekcPoint[\"experiences\"][\"done\"],\n",
    "                    )\n",
    "\n",
    "                # Backup the current file to avoide data loss in future read/writes (if reading is successful)\n",
    "                import shutil\n",
    "                shutil.copyfile(os.path.join(savePath, _file), os.path.join(savePath, _file.replace(\".pth\", \"_Backup.pth\")))\n",
    "                print(f\"Loaded network weights for episode {startEpisode}\")\n",
    "    else:\n",
    "        print(\"Save path doesn't exist. Making it.\")\n",
    "        os.makedirs(savePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db4faa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElapsedTime: 89   s | Episode: 206   | Timestep: 177   | The average of the 100   episodes is: -84  \n",
      "Latest chekpoint: 0 | Speed 308.1 tps | ebsilon: 0.356\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 30>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     66\u001b[0m     experience \u001b[38;5;241m=\u001b[39m mem\u001b[38;5;241m.\u001b[39msample(miniBatchSize)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# Update the Q-Network and the target Q-Network\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# Bear in mind that we do not update the target Q-network with direct gradient descent.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# so there is no optimizer needed for it\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m     \u001b[43mfitQNetworks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mqNetwork_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_main\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtargetQNetwork_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Save the necessary data\u001b[39;00m\n\u001b[0;32m     74\u001b[0m points \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[1;32mc:\\Users\\Spino.shop\\Desktop\\Trading\\Apps\\Python_29-_RL_SNN\\ANN_DQN\\utils.py:218\u001b[0m, in \u001b[0;36mfitQNetworks\u001b[1;34m(experience, gamma, qNetwork, target_qNetwork)\u001b[0m\n\u001b[0;32m    215\u001b[0m __targetQNetworkModel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    217\u001b[0m __qNetworkOptim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 218\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m __qNetworkOptim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    221\u001b[0m \u001b[38;5;66;03m# Update the target Q network's weights using soft updating method\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Spino.shop\\Desktop\\Trading\\Apps\\.MotherVenv\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Spino.shop\\Desktop\\Trading\\Apps\\.MotherVenv\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Spino.shop\\Desktop\\Trading\\Apps\\.MotherVenv\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"Device is: {__device}\")\n",
    "\n",
    "# Start the timer\n",
    "tstart = time.time()\n",
    "\n",
    "# The experience of the agent is saved as a named tuple containing various variables\n",
    "agentExp = namedtuple(\"exp\", [\"state\", \"action\", \"reward\", \"nextState\", \"done\"])\n",
    "\n",
    "# Parameters\n",
    "nEpisodes = 6000 # Number of learning episodes\n",
    "maxNumTimeSteps = 1000 # The number of time step in each episode\n",
    "ebsilon = 1 if startEbsilon == None else startEbsilon # The starting  value of ebsilon\n",
    "ebsilonEnd   = .1 # The finishing value of ebsilon\n",
    "eDecay = eDecay # The rate at which ebsilon decays\n",
    "numUpdateTS = 4 # Frequency of time steps to update the NNs\n",
    "numP_Average = 100 # The number of previous episodes for calculating the average episode reward\n",
    "\n",
    "# Variables for saving the required data for later analysis\n",
    "episodePointHist = [] # For saving each episode's point for later demonstration\n",
    "episodeTimeHist = [] # For saving the time it took for episode to end\n",
    "actionString = \"\" # A string containing consecutive actions taken in an episode (dellimited by comma, i.e. 1,2,4,2,1 etc.)\n",
    "episodeHistDf = None\n",
    "lstHistory = [] if lstHistory == None else lstHistory\n",
    "initialCond = None # initial condition (state) of the episode\n",
    "epPointAvg = -999999 if len(lstHistory) == 0 else pd.DataFrame(lstHistory).iloc[-numP_Average:][\"points\"].mean()\n",
    "latestChekpoint = 0\n",
    "_lastPrinttime = 0\n",
    "\n",
    "\n",
    "for episode in range(startEpisode, nEpisodes):\n",
    "    initialSeed = random.randint(1,1_000_000_000) # The random seed that determines the episode's I.C.\n",
    "    state, info = env.reset(seed = initialSeed)\n",
    "    points = 0\n",
    "    actionString = \"\"\n",
    "    initialCond = state\n",
    "\n",
    "    tempTime = time.time()\n",
    "    \n",
    "    for t in range(maxNumTimeSteps):\n",
    "\n",
    "        qValueForActions = qNetwork_model(torch.tensor(state, device = __device, dtype = __dtype))\n",
    "\n",
    "        # use ebsilon-Greedy algorithm to take the new step\n",
    "        action = getAction(qValueForActions, ebsilon)\n",
    "\n",
    "        # Take a step\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # Store the experience of the current step in an experience deque.\n",
    "        mem.addNew(\n",
    "            agentExp(\n",
    "                state, # Current state\n",
    "                action,\n",
    "                reward, # Current state's reward\n",
    "                observation, # Next state\n",
    "                True if terminated or truncated else False\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Check to see if we have to update the networks in the current step\n",
    "        update = updateNetworks(t, mem, miniBatchSize, numUpdateTS)\n",
    "\n",
    "        if update:\n",
    "            # Update the NNs\n",
    "            experience = mem.sample(miniBatchSize)\n",
    "\n",
    "            # Update the Q-Network and the target Q-Network\n",
    "            # Bear in mind that we do not update the target Q-network with direct gradient descent.\n",
    "            # so there is no optimizer needed for it\n",
    "            fitQNetworks(experience, gamma, [qNetwork_model, optimizer_main], [targetQNetwork_model, None])\n",
    "\n",
    "        # Save the necessary data\n",
    "        points += reward\n",
    "        state = observation.copy()\n",
    "        actionString += f\"{action},\"\n",
    "        # Print the training status. Print only once each second to avoid jitters.\n",
    "        if 1 < (time.time() - _lastPrinttime):\n",
    "            clear_output(wait=True)\n",
    "            _lastPrinttime = time.time()\n",
    "            print(f\"ElapsedTime: {int(time.time() - tstart): <5}s | Episode: {episode: <5} | Timestep: {t: <5} | The average of the {numP_Average: <5} episodes is: {int(epPointAvg): <5}\")\n",
    "            print(f\"Latest chekpoint: {latestChekpoint} | Speed {t/(time.time()-tempTime+1e-9):.1f} tps | ebsilon: {ebsilon:.3f}\")\n",
    "\n",
    "            # fig= plt.figure(figsize=(12,6))\n",
    "            # plt.plot(pd.DataFrame(lstHistory)[\"episode\"], pd.DataFrame(lstHistory)[\"points\"])\n",
    "            # plt.show()\n",
    "\n",
    "        # Handle episode ending\n",
    "        if terminated or truncated:\n",
    "            # Save the episode history in dataframe\n",
    "            if (episode+1) % 3 == 0:\n",
    "                # only save every 10 episodes\n",
    "                lstHistory.append({\n",
    "                    \"episode\": episode,\n",
    "                    \"seed\": initialSeed,\n",
    "                    \"points\": points,\n",
    "                    \"timesteps\": t,\n",
    "                    \"duration\": time.time() - tempTime\n",
    "                })\n",
    "                \n",
    "            break\n",
    "\n",
    "    # Saving the current episode's points and time\n",
    "    episodePointHist.append(points)\n",
    "    episodeTimeHist.append(time.time()-tempTime)\n",
    "\n",
    "    # Getting the average of {numP_Average} episodes\n",
    "    epPointAvg = np.mean(episodePointHist[-numP_Average:])\n",
    "\n",
    "    # Decay ebsilon\n",
    "    ebsilon = decayEbsilon(ebsilon, eDecay, ebsilonEnd)\n",
    "\n",
    "    # Save model weights and parameters periodically (For later use)\n",
    "    if backUpNetworks:\n",
    "        if (episode + 1) % 20 == 0:\n",
    "            _exp = mem.exportExpereince()\n",
    "            _chekcPoint = {\n",
    "                \"episode\": episode,\n",
    "                'qNetwork_state_dict': qNetwork_model.state_dict(),\n",
    "                'qNetwork_optimizer_state_dict': optimizer_main.state_dict(),\n",
    "                'targetQNetwork_state_dict': targetQNetwork_model.state_dict(),\n",
    "                'targetQNetwork_optimizer_state_dict': optimizer_target.state_dict(),\n",
    "                'hyperparameters': {\"ebsilon\": ebsilon, \"eDecay\":eDecay},\n",
    "                \"train_history\": lstHistory,\n",
    "                \"experiences\": {\n",
    "                    \"state\": _exp[\"state\"],\n",
    "                    \"action\": _exp[\"action\"],\n",
    "                    \"reward\": _exp[\"reward\"],\n",
    "                    \"nextState\": _exp[\"nextState\"],\n",
    "                    \"done\": _exp[\"done\"]\n",
    "                }\n",
    "            }\n",
    "            qNetworkSaveHistory.appendleft(_chekcPoint)\n",
    "            torch.save(qNetworkSaveHistory, os.path.join(savePath, f\"{projectName}_{modelDetails}.pth\"))\n",
    "\n",
    "            # Save the episode number\n",
    "            latestChekpoint = episode\n",
    "\n",
    "    # Stop the learning process if suitable average point is reacheds\n",
    "    if 100 < epPointAvg:\n",
    "        Tend = time.time()\n",
    "        print(f\"\\nThe learning ended. Elapsed time for learning: {Tend-tstart:.2f}s. \\nAVG of latest 100 episodes: {epPointAvg}\")\n",
    "        \n",
    "        _exp = mem.exportExpereince()\n",
    "        _chekcPoint = {\n",
    "            \"episode\": episode,\n",
    "            'qNetwork_state_dict': qNetwork_model.state_dict(),\n",
    "            'qNetwork_optimizer_state_dict': optimizer_main.state_dict(),\n",
    "            'targetQNetwork_state_dict': targetQNetwork_model.state_dict(),\n",
    "            'targetQNetwork_optimizer_state_dict': optimizer_target.state_dict(),\n",
    "            'hyperparameters': {\"ebsilon\": ebsilon, \"eDecay\":eDecay},\n",
    "            \"train_history\": lstHistory,\n",
    "            \"experiences\": {\n",
    "                \"state\": _exp[\"state\"],\n",
    "                \"action\": _exp[\"action\"],\n",
    "                \"reward\": _exp[\"reward\"],\n",
    "                \"nextState\": _exp[\"nextState\"],\n",
    "                \"done\": _exp[\"done\"]\n",
    "            }\n",
    "        }\n",
    "        qNetworkSaveHistory.appendleft(_chekcPoint)\n",
    "        torch.save(qNetworkSaveHistory, os.path.join(savePath, f\"{projectName}_{modelDetails}.pth\"))\n",
    "\n",
    "        # Save the episode number\n",
    "        latestChekpoint = episode\n",
    "        \n",
    "        break\n",
    "\n",
    "# Reset the index\n",
    "episodeHistDf = pd.DataFrame(lstHistory)\n",
    "episodeHistDf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3991cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig= plt.figure(figsize=(12,6))\n",
    "plt.plot(pd.DataFrame(lstHistory)[\"episode\"], pd.DataFrame(lstHistory)[\"points\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".MotherVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
