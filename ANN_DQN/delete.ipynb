{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a374574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "\n",
    "def loadNetwork(fileName, **kwargs):\n",
    "    \"\"\"\n",
    "    Loads the previous training details from a file. The file should be \n",
    "    a dictionary, with all the details necessary to pickup where you left.\n",
    "    The necessary data are explained below.\n",
    "    \n",
    "    Args:\n",
    "        fileName (str): The name of the file\n",
    "        kwargs (dict): A dictionary with the following keys:\n",
    "            qNetwork_model (torch.nn): The qNetwork_model\n",
    "            optimizer_main (torch.optim): The qNetwork_model's optimizer \n",
    "                object\n",
    "            targetQNetwork_model (torch.nn): The targetQNetwork_model\n",
    "            trainingParams (list): A list containing following parameters\n",
    "                in order. We chose this approach to be able to change training \n",
    "                parameters in-place:\n",
    "                startEpisode (int): The episode number to start from\n",
    "                startEbsilon (int): The starting ebsilon number (The ebsilon \n",
    "                    prior to latest run's termination)\n",
    "                lstHistory (list): The list holding the training history\n",
    "                eDecay (float): The decay of ebsilon\n",
    "                mem (ReplayMemory): An instance of replay memory object\n",
    "    \"\"\"\n",
    "    # Check if all necessary data has been given so it can be overwritten \n",
    "    # when loaded and passed back to the user\n",
    "    assert \"qNetwork_model\" in kwargs.keys(), \"Please pass the qNetwork_model object\"\n",
    "    assert \"optimizer_main\" in kwargs.keys(), \"Please pass the optimizer_main object\"\n",
    "    assert \"targetQNetwork_model\" in kwargs.keys(), \"Please pass the targetQNetwork_model object\"\n",
    "    assert \"trainingParams\" in kwargs.keys(), \"Please pass the trainingParams object\"\n",
    "    assert len(kwargs[\"trainingParams\"]) == 5, \"You should enter the following parameters in the order:\\nstartEpisode, startEbsilon, lstHistory, eDecay, mem\"\n",
    "    \n",
    "    if os.path.isfile(fileName):\n",
    "        try:\n",
    "            # Try to read the main file\n",
    "            try:\n",
    "                __data = torch.load(fileName, weights_only = False)\n",
    "            except:\n",
    "                print(\"Couldn't load the main file, trying to load the backup file\")\n",
    "                try:\n",
    "                    # Try to read the backup file\n",
    "                    __data = torch.load(os.path.join(os.path.dirname(fileName), \"Backups\", os.path.basename(fileName)), weights_only = False)\n",
    "                except Exception as e:\n",
    "                    raise Exception(f\"Couldn't load the backup file\")\n",
    "            \n",
    "            # Check if the file is a dictionary\n",
    "            if not isinstance(__data, dict): raise Exception(f\"Couldn't load the file. File {fileName} is not a dictionary\")\n",
    "\n",
    "            # Load Q-Network\n",
    "            kwargs[\"qNetwork_model\"].load_state_dict(__data[\"qNetwork_state_dict\"]) # Model weights\n",
    "            kwargs[\"optimizer_main\"].load_state_dict(__data[\"qNetwork_optimizer_state_dict\"]) # Optimizer\n",
    "\n",
    "            # Load target Q-Network\n",
    "            kwargs[\"targetQNetwork_model\"].load_state_dict(__data[\"targetQNetwork_state_dict\"]) # Model weights\n",
    "\n",
    "            # Load process parameters\n",
    "            kwargs[\"trainingParams\"][0] = __data[\"episode\"] # Starting episode number\n",
    "            kwargs[\"trainingParams\"][1] = __data[\"hyperparameters\"][\"ebsilon\"] # Starting ebsilon\n",
    "            kwargs[\"trainingParams\"][2] = __data[\"train_history\"]\n",
    "            kwargs[\"trainingParams\"][3] = __data[\"hyperparameters\"][\"eDecay\"]\n",
    "\n",
    "            kwargs[\"trainingParams\"][4].loadExperiences(\n",
    "                __data[\"experiences\"][\"state\"],\n",
    "                __data[\"experiences\"][\"action\"],\n",
    "                __data[\"experiences\"][\"reward\"],\n",
    "                __data[\"experiences\"][\"nextState\"],\n",
    "                __data[\"experiences\"][\"done\"],\n",
    "            )\n",
    "            \n",
    "            # All changes are in-place, however, we return the changed objects for convenience\n",
    "            return (\n",
    "                kwargs[\"qNetwork_model\"],\n",
    "                kwargs[\"optimizer_main\"],\n",
    "                kwargs[\"targetQNetwork_model\"],\n",
    "                kwargs[\"trainingParams\"][0],  # startEpisode\n",
    "                kwargs[\"trainingParams\"][1],  # startEbsilon\n",
    "                kwargs[\"trainingParams\"][2],  # lstHistory\n",
    "                kwargs[\"trainingParams\"][3],  # eDecay\n",
    "                kwargs[\"trainingParams\"][4]   # mem\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"ERROR: \", e)\n",
    "            return None\n",
    "    else:\n",
    "        raise Exception(f\"Couldn't load the file. File {fileName} does not exist\")\n",
    "\n",
    "def modelParamParser():\n",
    "    \"\"\"\n",
    "    Gets the arguments from the command line for the model to run\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description = \"The neural network with specified parameters\")\n",
    "    parser.add_argument(\"--name\", \"-n\", type = str, default = \"parallelDQN\", help = \"The name of the model\")\n",
    "    parser.add_argument(\"--continue_run\", \"-c\", type = bool, default = False, help = \"Continue the last run\")\n",
    "    parser.add_argument(\"--hidden_layers\", \"-hl\", type = int, nargs = \"+\", default = [64, 64], help = \"Hidden layer size\")\n",
    "    parser.add_argument(\"--learning_rate\", \"-lr\", type = float, default = 0.0001, help = \"Learning rate\")\n",
    "    parser.add_argument(\"--decay\", \"-d\", type = float, default = 0.999, help = \"Ebsilon decay rate\")\n",
    "    parser.add_argument(\"--batch\", \"-b\", type = int, default = 1000, help = \"The mini-batch size\")\n",
    "    parser.add_argument(\"--gamma\", \"-g\", type = float, default = 0.995, help = \"Discount factor\")\n",
    "    parser.add_argument(\"--extra_info\", \"-extra\", type = str, default = \"\", help = \"Extra information\")\n",
    "    \n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "edb98c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import qNetwork_ANN\n",
    "from collections import deque, namedtuple\n",
    "import os\n",
    "import argparse\n",
    "from utils import *\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random, imageio, time, copy\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Define the super parameters\n",
    "projectName = \"parallelDQN\"\n",
    "\n",
    "# Save/Get weights from persistent storage. Pass empty string for not saving. \n",
    "# Pass drive for using google derive (If code is running in colab). If local, \n",
    "# pass the location of your desire\n",
    "savePath = \"./Data\"\n",
    "continueLastRun = False\n",
    "backUpData = {}\n",
    "\n",
    "# Make the save directory if it does not exist\n",
    "os.makedirs(savePath, exist_ok = True)\n",
    "\n",
    "# Making the environments\n",
    "NUM_ENVS = 2\n",
    "env = gym.make(\"LunarLander-v3\") # Use render_mode = \"human\" to render each episode\n",
    "envs = gym.make_vec(\n",
    "    \"LunarLander-v3\", \n",
    "    num_envs = NUM_ENVS, # Number of environments to create\n",
    "    vectorization_mode = \"async\",\n",
    "    wrappers = (gym.wrappers.TimeAwareObservation,),\n",
    ")\n",
    "states, info = env.reset() # Get a sample state of the environment\n",
    "stateSize = env.observation_space.shape # Number of variables to define current step\n",
    "nActions = env.action_space.n # Number of actions\n",
    "actionSpace = np.arange(nActions).tolist()\n",
    "\n",
    "# Set pytorch parameters: The device (CPU or GPU) and data types\n",
    "__device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "__dtype = torch.float\n",
    "\n",
    "# Model parameters\n",
    "parser = modelParamParser()\n",
    "args, unknown = parser.parse_known_args()\n",
    "hiddenNodes = args.hidden_layers\n",
    "learningRate = args.learning_rate\n",
    "eDecay = args.decay\n",
    "miniBatchSize = args.batch # The length of mini-batch that is used for training\n",
    "gamma = args.gamma # The discount factor\n",
    "extraInfo = args.extra_info\n",
    "continueLastRun = args.continue_run\n",
    "\n",
    "# handle the save location\n",
    "modelDetails = f\"{'_'.join([str(l) for l in hiddenNodes])}_{learningRate}_{eDecay}_{miniBatchSize}_{gamma}_{extraInfo}\"\n",
    "savePath = os.path.join(savePath, f\"{projectName}_{modelDetails}\")\n",
    "os.makedirs(savePath, exist_ok=True)\n",
    "\n",
    "# Get how many times the model has been trained and add it to the file name\n",
    "runNumber =  len([f for f in os.listdir(savePath) if f\"{modelDetails}\" in f and os.path.isfile(os.path.join(savePath, f))]) if savePath != None else \"\"\n",
    "modelDetails += f\"_{runNumber}\" if not continueLastRun else f\"_{runNumber -1 }\"\n",
    "saveFileName = f\"{projectName}_{modelDetails}.pth\"\n",
    "\n",
    "# Make the model objects\n",
    "qNetwork_model = qNetwork_ANN([stateSize[0], *hiddenNodes, nActions]).to(__device, dtype = __dtype)\n",
    "targetQNetwork_model = qNetwork_ANN([stateSize[0], *hiddenNodes, nActions]).to(__device, dtype = __dtype)\n",
    "\n",
    "# Two models should have identical weights initially\n",
    "targetQNetwork_model.load_state_dict(qNetwork_model.state_dict())\n",
    "\n",
    "# TODO: Add gradient clipping to the optimizer for avoiding exploding gradients\n",
    "# Suitable optimizer for gradient descent\n",
    "optimizer_main = torch.optim.Adam(qNetwork_model.parameters(), lr=learningRate)\n",
    "optimizer_target = torch.optim.Adam(targetQNetwork_model.parameters(), lr=learningRate)\n",
    "\n",
    "# Starting episode and ebsilon\n",
    "startEpisode = 0\n",
    "startEbsilon = None\n",
    "lstHistory = None\n",
    "\n",
    "# Making the memory buffer object\n",
    "memorySize = 100_000 # The length of the entire memory\n",
    "mem = ReplayMemory(memorySize, __dtype, __device)\n",
    "\n",
    "if continueLastRun and os.path.isfile(os.path.join(savePath, saveFileName)):\n",
    "    # Load necessary parameters to resume the training from most recent run \n",
    "    saveLen = 1\n",
    "    load_params = {\n",
    "        \"qNetwork_model\": qNetwork_model,\n",
    "        \"optimizer_main\": optimizer_main,\n",
    "        \"targetQNetwork_model\": targetQNetwork_model,\n",
    "        \"trainingParams\": [startEpisode, startEbsilon, lstHistory, eDecay, mem]\n",
    "    }\n",
    "    qNetwork_model, optimizer_main, targetQNetwork_model, startEpisode, startEbsilon, lstHistory, eDecay, mem =loadNetwork(os.path.join(savePath, saveFileName), **load_params)\n",
    "    print(\"Continuing from episode:\", startEpisode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec13b761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is: cuda\n",
      "ElapsedTime: 0    s | Episode: 0     | Time step: 0     | The average of the 100   episodes is: -999999\n",
      "Latest checkpoint: 0 | Speed 0.0 tps | ebsilon: 1.000\n",
      "ElapsedTime: 1    s | Episode: 8     | Time step: 453   | The average of the 100   episodes is: -204 \n",
      "Latest checkpoint: 0 | Speed 418.2 tps | ebsilon: 0.992\n",
      "ElapsedTime: 2    s | Episode: 16    | Time step: 747   | The average of the 100   episodes is: -197 \n",
      "Latest checkpoint: 0 | Speed 352.1 tps | ebsilon: 0.984\n",
      "saving in  ./Data\\parallelDQN_64_64_0.0001_0.999_1000_0.995_\\parallelDQN_64_64_0.0001_0.999_1000_0.995__1.pth 19\n",
      "ElapsedTime: 3    s | Episode: 21    | Time step: 979   | The average of the 100   episodes is: -194 \n",
      "Latest checkpoint: 19 | Speed 307.9 tps | ebsilon: 0.979\n"
     ]
    }
   ],
   "source": [
    "print(f\"Device is: {__device}\")\n",
    "\n",
    "# Start the timer\n",
    "tstart = time.time()\n",
    "\n",
    "# The experience of the agent is saved as a named tuple containing various variables\n",
    "agentExp = namedtuple(\"exp\", [\"state\", \"action\", \"reward\", \"nextState\", \"done\"])\n",
    "\n",
    "# Parameters\n",
    "nEpisodes = 6000 # Number of learning episodes\n",
    "maxNumTimeSteps = 1000 # The number of time step in each episode\n",
    "ebsilon = 1 if startEbsilon == None else startEbsilon # The starting  value of ebsilon\n",
    "ebsilonEnd   = .1 # The finishing value of ebsilon\n",
    "eDecay = eDecay # The rate at which ebsilon decays\n",
    "numUpdateTS = 4 # Frequency of time steps to update the NNs\n",
    "numP_Average = 100 # The number of previous episodes for calculating the average episode reward\n",
    "\n",
    "# Variables for saving the required data for later analysis\n",
    "episodePointHist = [] # For saving each episode's point for later demonstration\n",
    "episodeHistDf = None\n",
    "lstHistory = [] if lstHistory == None else lstHistory\n",
    "initialCond = None # Initial condition (state) of the episode\n",
    "epPointAvg = -999999 if len(lstHistory) == 0 else pd.DataFrame(lstHistory).iloc[-numP_Average:][\"points\"].mean()\n",
    "latestCheckpoint = 0\n",
    "_lastPrintTime = 0\n",
    "\n",
    "\n",
    "initialSeed = random.randint(1,1_000_000_000) # The random seed that determines the episode's I.C.\n",
    "states, info = envs.reset(seed = initialSeed)\n",
    "points = np.zeros((envs.num_envs, 1))\n",
    "initialCond = states\n",
    "tempTime = time.time()\n",
    "t = 0\n",
    "episode = 0\n",
    "\n",
    "while True:\n",
    "    # The last element of each state is the time step, so we slice the tensor\n",
    "    qValueForActions = qNetwork_model(torch.tensor(states[:,:-1], device = __device, dtype = __dtype))\n",
    "\n",
    "    # use ebsilon-Greedy algorithm to take the new step\n",
    "    action = getAction(qValueForActions, ebsilon, actionSpace, __device).cpu().numpy()\n",
    "\n",
    "    # Take a step\n",
    "    observation, reward, terminated, truncated, _ = envs.step(action)\n",
    "\n",
    "    batchExperiences = [agentExp(s, a, r, o, d) for s, a, r, o, d in zip(states[:,:-1], action, reward, observation[:,:-1], terminated | truncated) ]\n",
    "\n",
    "    # Store the experience of the current step in an experience deque.\n",
    "    mem.addMultiple(batchExperiences)\n",
    "\n",
    "    # Check to see if we have to update the networks in the current step\n",
    "    update = updateNetworks(t, mem, miniBatchSize, numUpdateTS)\n",
    "\n",
    "    if update:\n",
    "        # Update the NNs\n",
    "        experience = mem.sample(miniBatchSize)\n",
    "\n",
    "        # Update the Q-Network and the target Q-Network\n",
    "        # Bear in mind that we do not update the target Q-network with direct gradient descent.\n",
    "        # so there is no optimizer needed for it\n",
    "        fitQNetworks(experience, gamma, [qNetwork_model, optimizer_main], [targetQNetwork_model, None])\n",
    "\n",
    "    # Save the necessary data\n",
    "    points += reward.reshape(-1, 1)\n",
    "    states = observation.copy()\n",
    "\n",
    "    # Print the training status. Print only once each second to avoid jitters.\n",
    "    if 1 < (time.time() - _lastPrintTime):\n",
    "        os.system('clear')\n",
    "        _lastPrintTime = time.time()\n",
    "        print(f\"ElapsedTime: {int(time.time() - tstart): <5}s | Episode: {episode: <5} | Time step: {t: <5} | The average of the {numP_Average: <5} episodes is: {int(epPointAvg): <5}\")\n",
    "        print(f\"Latest checkpoint: {latestCheckpoint} | Speed {t/(time.time()-tempTime+1e-9):.1f} tps | ebsilon: {ebsilon:.3f}\")\n",
    "\n",
    "    # Handle episode ending\n",
    "    if (terminated | truncated).any():\n",
    "        mask = terminated | truncated\n",
    "        finalPoint = points[mask]\n",
    "        \n",
    "        for k in range(finalPoint.shape[0]):\n",
    "            # Decay ebsilon\n",
    "            ebsilon = decayEbsilon(ebsilon, eDecay, ebsilonEnd)\n",
    "            episode += 1\n",
    "            \n",
    "            # Save the episode history in dataframe\n",
    "            if (episode+1) % 3 == 0:\n",
    "                # only save every 10 episodes\n",
    "                lstHistory.append({\n",
    "                    \"episode\": episode,\n",
    "                    \"seed\": initialSeed,\n",
    "                    \"points\": finalPoint[k]\n",
    "                })\n",
    "            \n",
    "            # Save model weights and parameters periodically (For later use)\n",
    "            if (episode + 1) % (20) == 0:\n",
    "                _exp = mem.exportExperience()\n",
    "                backUpData = {\n",
    "                    \"episode\": episode,\n",
    "                    'qNetwork_state_dict': qNetwork_model.state_dict(),\n",
    "                    'qNetwork_optimizer_state_dict': optimizer_main.state_dict(),\n",
    "                    'targetQNetwork_state_dict': targetQNetwork_model.state_dict(),\n",
    "                    'targetQNetwork_optimizer_state_dict': optimizer_target.state_dict(),\n",
    "                    'hyperparameters': {\"ebsilon\": ebsilon, \"eDecay\":eDecay},\n",
    "                    \"train_history\": lstHistory,\n",
    "                    \"experiences\": {\n",
    "                        \"state\": _exp[\"state\"],\n",
    "                        \"action\": _exp[\"action\"],\n",
    "                        \"reward\": _exp[\"reward\"],\n",
    "                        \"nextState\": _exp[\"nextState\"],\n",
    "                        \"done\": _exp[\"done\"]\n",
    "                    }\n",
    "                }\n",
    "                print(\"saving in \", os.path.join(savePath, saveFileName), episode)\n",
    "                saveModel(backUpData, os.path.join(savePath, saveFileName))\n",
    "\n",
    "                # Save the episode number\n",
    "                latestCheckpoint = episode\n",
    "        \n",
    "        # Add the points to the history\n",
    "        episodePointHist.extend(finalPoint.tolist())\n",
    "        \n",
    "        # Getting the average of {numP_Average} episodes\n",
    "        epPointAvg = np.mean(episodePointHist[-numP_Average:])\n",
    "\n",
    "        # Reset the points of terminated episodes\n",
    "        points[mask] = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    if episode == 25: break\n",
    "\n",
    "\n",
    "    # Stop the learning process if suitable average point is reached\n",
    "    if 200 < epPointAvg:\n",
    "        Tend = time.time()\n",
    "        print(f\"\\nThe learning ended. Elapsed time for learning: {Tend-tstart:.2f}s. \\nAVG of latest 100 episodes: {epPointAvg}\")\n",
    "        \n",
    "        _exp = mem.exportExperience()\n",
    "        backUpData = {\n",
    "            \"episode\": episode,\n",
    "            'qNetwork_state_dict': qNetwork_model.state_dict(),\n",
    "            'qNetwork_optimizer_state_dict': optimizer_main.state_dict(),\n",
    "            'targetQNetwork_state_dict': targetQNetwork_model.state_dict(),\n",
    "            'targetQNetwork_optimizer_state_dict': optimizer_target.state_dict(),\n",
    "            'hyperparameters': {\"ebsilon\": ebsilon, \"eDecay\":eDecay},\n",
    "            \"train_history\": lstHistory,\n",
    "            \"experiences\": {\n",
    "                \"state\": _exp[\"state\"],\n",
    "                \"action\": _exp[\"action\"],\n",
    "                \"reward\": _exp[\"reward\"],\n",
    "                \"nextState\": _exp[\"nextState\"],\n",
    "                \"done\": _exp[\"done\"]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        saveModel(backUpData, os.path.join(savePath, saveFileName))\n",
    "\n",
    "        # Save the episode number\n",
    "        latestCheckpoint = episode\n",
    "        \n",
    "        break\n",
    "    t += 1\n",
    "\n",
    "# Reset the index\n",
    "episodeHistDf = pd.DataFrame(lstHistory)\n",
    "episodeHistDf.reset_index(drop=True, inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".MotherVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
