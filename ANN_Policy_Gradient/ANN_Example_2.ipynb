{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN using gradient policy for training (Parallel training with multiple agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distributions\n",
    "import numpy as np\n",
    "import gymnasium as gym  \n",
    "import warnings\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the gymnasium environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Define the network\n",
    "class policyNetwork(nn.Module):\n",
    "    def __init__(self, inputDim, hiddenDim, outDim, dropOut):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(inputDim, hiddenDim)\n",
    "        self.layer2 = nn.Linear(hiddenDim, outDim)\n",
    "        self.dropout = nn.Dropout(dropOut)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.dropout(x1)\n",
    "        x3 = F.relu(x2)\n",
    "        x4 = self.layer2(x3)\n",
    "        return x4\n",
    "\n",
    "# Calculate the cumulative returns in a specific trajectory\n",
    "def cumulativeReturn(trajectoryRewards, gamma):\n",
    "    # gamma is the discount factor\n",
    "    \n",
    "    returns = []\n",
    "    __R = 0\n",
    "    \n",
    "    for reward in reversed(trajectoryRewards):\n",
    "        __R = reward + gamma * __R\n",
    "        returns.insert(0, __R)\n",
    "    \n",
    "    returns = torch.tensor(returns)\n",
    "    \n",
    "    # Normalize the returns\n",
    "    return (returns - returns.mean())/(returns.std() + 1e-9)\n",
    "\n",
    "def cumulativeReturn_vectorized(rewards, gamma):\n",
    "    \"\"\"\n",
    "    Compute stepwise returns with a discount factor using a vectorized approach in PyTorch.\n",
    "    Explanation link: https://x.com/i/grok/share/1SAiyt3UzyHHDIE6h3ALxCyz2\n",
    "    \n",
    "    Parameters:\n",
    "    - rewards: 1D torch tensor of shape (T,) containing the reward sequence [r_0, r_1, ..., r_{T-1}]\n",
    "    - gamma: float, discount factor between 0 and 1\n",
    "    \n",
    "    Returns:\n",
    "    - returns: 1D torch tensor of shape (T,) containing [G_0, G_1, ..., G_{T-1}]\n",
    "    \"\"\"\n",
    "    # Ensure rewards is a 1D tensor\n",
    "    assert rewards.dim() == 1, \"rewards must be a 1D tensor\"\n",
    "    # print(\"input\", rewards)\n",
    "    T = rewards.size(0)\n",
    "    # Create indices for broadcasting\n",
    "    indices = torch.arange(T)\n",
    "    # Compute exponents (j - i) using broadcasting\n",
    "    exponents = indices[None, :] - indices[:, None]\n",
    "    # Construct the upper triangular matrix A\n",
    "    A = torch.where(exponents >= 0, gamma ** exponents.float(), torch.zeros_like(exponents).float())\n",
    "    # Compute returns via matrix multiplication\n",
    "    returns = A @ rewards\n",
    "    return (returns - returns.mean())/(returns.std() + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 260 | Mean Rewards: 149.3 | tps: 845.95\n",
      "Reached reward threshold in 267 forward passes\n"
     ]
    }
   ],
   "source": [
    "# Synchronous environments (Network updates when all agents are done)\n",
    "# Works perfectly fine\n",
    "\n",
    "def forwardPass(envs, policyNetwork, discountFactor):\n",
    "    \"\"\"\n",
    "    Runs a forward pass through all environments and only after all of them are done\n",
    "    the pass finishes. After termination of each environment, the accumulation of \n",
    "    the environment's data stops; however, when envs.step(...) is ran, the terminated\n",
    "    environments are restarted and continued. This approach has been employed because \n",
    "    Gymnasium currently doesn't support running steps for particular environments, or\n",
    "    restarting particular environments.\n",
    "\n",
    "    Args:\n",
    "        envs (gymnasium vector environment): A gymnasium vector environment\n",
    "        policyNetwork (torch.nn): The network we are trying to learn\n",
    "        discountFactor (float): the discount factor to calculate rewards in a trajectory\n",
    "\n",
    "    Returns:\n",
    "        overallRewards: The accumulated reward of each agent's trajectory\n",
    "        trajectoryRewards: A list of pyTorch tensors\n",
    "        log_probActions: A list of pyTorch tensors\n",
    "    \"\"\"\n",
    "    log_probActions = [[] for _ in range(NUM_ENVS)]\n",
    "    rewards_hist = [[] for _ in range(NUM_ENVS)]\n",
    "    \n",
    "    activeEpisodes =  torch.ones(NUM_ENVS, dtype = torch.bool)\n",
    "    \n",
    "    states, _ = envs.reset(seed = random.randint(1,1_000_000_000))\n",
    "    \n",
    "    while True:\n",
    "        actionPred = policyNetwork(torch.Tensor(states[:,:4]))\n",
    "        actionProb = F.softmax(actionPred, dim = -1)\n",
    "        dist = distributions.Categorical(actionProb)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        states, rewards, terminated, truncated, _ = envs.step(action.numpy())\n",
    "\n",
    "        # Gather logarithm of actions and rewards for active envs (envs that are \n",
    "        # not terminated or truncated)\n",
    "        for idx in range(NUM_ENVS):\n",
    "            # Only add history if the episode has not been truncated\n",
    "            if activeEpisodes[idx]:\n",
    "                log_probActions[idx].append(dist.log_prob(action).unsqueeze(-1)[0])\n",
    "                rewards_hist[idx].append(torch.tensor(rewards[idx], dtype = torch.float).unsqueeze(-1))\n",
    "\n",
    "        # If an env is terminated or truncated, take it off the active env list\n",
    "        if ((terminated | truncated).any()):\n",
    "            _terminated = np.where((terminated | truncated) == True)[0]\n",
    "            for idx in _terminated: activeEpisodes[idx] = False\n",
    "        \n",
    "        # If all envs are terminated or truncated, stop the forward pass\n",
    "        if not activeEpisodes.any():\n",
    "            overallRewards = [torch.tensor(rewards_hist[idx], dtype = torch.float).sum().item() for idx in range(NUM_ENVS)]\n",
    "            trajectoryRewards = [cumulativeReturn_vectorized(torch.cat(rewards_hist[idx]), discountFactor) for idx in range(NUM_ENVS)]\n",
    "            log_probActions = [torch.cat(log_probActions[idx]) for idx in range(NUM_ENVS)]\n",
    "            break\n",
    "    \n",
    "    return overallRewards, trajectoryRewards, log_probActions\n",
    "\n",
    "\n",
    "NUM_ENVS = 1\n",
    "envs = gym.make_vec(\n",
    "    \"CartPole-v1\", \n",
    "    num_envs = NUM_ENVS, # Number of environments to create\n",
    "    vectorization_mode = \"async\",\n",
    "    wrappers = (gym.wrappers.TimeAwareObservation,),\n",
    ")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Async version with forward pass as a single function.\n",
    "# Hyperparameters\n",
    "MAX_EPOCHS = 5000\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "N_TRIALS = 25\n",
    "REWARD_THRESHOLD = 200\n",
    "PRINT_INTERVAL = 10\n",
    "INPUT_DIM = env.observation_space.shape[0]\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = env.action_space.n\n",
    "DROPOUT = 0.5\n",
    "\n",
    "episodeReturns = []\n",
    "policy = policyNetwork(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT)\n",
    "optimizer = optim.Adam(policy.parameters(), lr = 0.01)\n",
    "\n",
    "\n",
    "mean_episode_return = 0\n",
    "episode = 0\n",
    "\n",
    "states, _ = envs.reset()\n",
    "overallTimeStep = 0\n",
    "# Setup the environment\n",
    "policy.train()\n",
    "_startTime = time.time()\n",
    "while True:\n",
    "    overallRewards, trajectoryRewards, log_probActions = forwardPass(envs, policy, DISCOUNT_FACTOR)\n",
    "    \n",
    "    _loss = 0\n",
    "    # Compute loss\n",
    "    for idx in range(NUM_ENVS):\n",
    "        _loss += -(log_probActions[idx] * trajectoryRewards[idx]).sum()\n",
    "    \n",
    "    __loss = _loss / NUM_ENVS\n",
    "    optimizer.zero_grad()\n",
    "    __loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    for _ep in overallRewards: episodeReturns.append(_ep)\n",
    "    mean_episode_return = np.mean(episodeReturns[-N_TRIALS:])\n",
    "\n",
    "    if episode % PRINT_INTERVAL == 0:\n",
    "        print(f'Episode: {episode:3} | Mean Rewards: {mean_episode_return:5.1f} | tps: {overallTimeStep/(time.time()-_startTime):.2f}', end=\"\\r\")\n",
    "\n",
    "    if mean_episode_return >= REWARD_THRESHOLD:\n",
    "        print(f'\\nReached reward threshold in {episode} forward passes')\n",
    "        break\n",
    "    \n",
    "    episode = episode + 1\n",
    "    overallTimeStep = overallTimeStep + max(map(len,trajectoryRewards))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".MotherVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
