{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN using gradient policy for training (Training with single agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distributions\n",
    "import numpy as np\n",
    "import gymnasium as gym  \n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the gymnasium environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Define the network\n",
    "class policyNetwork(nn.Module):\n",
    "    def __init__(self, inputDim, hiddenDim, outDim, dropOut):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(inputDim, hiddenDim)\n",
    "        self.layer2 = nn.Linear(hiddenDim, outDim)\n",
    "        self.dropout = nn.Dropout(dropOut)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "# Calculate the cumulative returns in a specific trajectory\n",
    "def cumulativeReturn(trajectoryRewards, gamma):\n",
    "    # gamma is the discount factor\n",
    "    \n",
    "    returns = []\n",
    "    __R = 0\n",
    "    \n",
    "    for reward in reversed(trajectoryRewards):\n",
    "        __R = reward + gamma * __R\n",
    "        returns.insert(0, __R)\n",
    "    \n",
    "    returns = torch.tensor(returns)\n",
    "    \n",
    "    # Normalize the returns\n",
    "    return (returns - returns.mean())/returns.std()\n",
    "\n",
    "def forwardPass(env, policyNetwork, gamma):\n",
    "    log_probActions = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    episodeReward = 0\n",
    "    \n",
    "    # Setup the environment\n",
    "    policyNetwork.train()\n",
    "    initialSeed = random.randint(1,1_000_000_000) # The random seed that determines the episode's I.C.\n",
    "    state, _ = env.reset(seed = initialSeed)\n",
    "    \n",
    "    while not done:\n",
    "        actionPred = policyNetwork(torch.Tensor(state).unsqueeze(0))\n",
    "        actionProb = F.softmax(actionPred, dim = -1)\n",
    "        dist = distributions.Categorical(actionProb)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        state, reward, terminated, truncated, info = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        log_probActions.append(dist.log_prob(action))\n",
    "        rewards.append(reward)\n",
    "        episodeReward += reward\n",
    "        \n",
    "    log_probActions = torch.cat(log_probActions)\n",
    "    trajectoryRewards = cumulativeReturn(rewards, gamma)\n",
    "    \n",
    "    return episodeReward, trajectoryRewards, log_probActions\n",
    "\n",
    "def computeLoss(log_probActions, trajectoryRewards):\n",
    "    return -(log_probActions * trajectoryRewards).sum()\n",
    "\n",
    "def updatePolicyNetwork(log_probActions, trajectoryRewards, optimizer):\n",
    "    __loss = computeLoss(log_probActions, trajectoryRewards.detach())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    __loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Episode:   0 | Mean Rewards:  38.0 | tps: 0.0\n",
      "| Episode:  10 | Mean Rewards:  18.6 | tps: 1032.2217368566771\n",
      "| Episode:  20 | Mean Rewards:  34.0 | tps: 1142.3422615887928\n",
      "| Episode:  30 | Mean Rewards:  47.8 | tps: 1169.434118242\n",
      "| Episode:  40 | Mean Rewards:  51.5 | tps: 1163.206235116598\n",
      "| Episode:  50 | Mean Rewards:  49.6 | tps: 1166.435724696814\n",
      "| Episode:  60 | Mean Rewards:  50.9 | tps: 1167.669059707102\n",
      "| Episode:  70 | Mean Rewards:  62.6 | tps: 1173.7855120975498\n",
      "| Episode:  80 | Mean Rewards: 115.8 | tps: 1129.816121697737\n",
      "| Episode:  90 | Mean Rewards: 143.9 | tps: 1231.0990144816208\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m _startTime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(MAX_EPOCHS):\n\u001b[1;32m---> 19\u001b[0m     episodeReward, trajectoryRewards, log_probActions \u001b[38;5;241m=\u001b[39m \u001b[43mforwardPass\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDISCOUNT_FACTOR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     updatePolicyNetwork(log_probActions, trajectoryRewards, optimizer)\n\u001b[0;32m     22\u001b[0m     episodeReturns\u001b[38;5;241m.\u001b[39mappend(episodeReward)\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mforwardPass\u001b[1;34m(env, policyNetwork, gamma)\u001b[0m\n\u001b[0;32m     47\u001b[0m actionPred \u001b[38;5;241m=\u001b[39m policyNetwork(torch\u001b[38;5;241m.\u001b[39mTensor(state)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     48\u001b[0m actionProb \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(actionPred, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactionProb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m     52\u001b[0m state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\Spino.shop\\Desktop\\Trading\\Apps\\.MotherVenv\\lib\\site-packages\\torch\\distributions\\categorical.py:72\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     69\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[0;32m     71\u001b[0m )\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Spino.shop\\Desktop\\Trading\\Apps\\.MotherVenv\\lib\\site-packages\\torch\\distributions\\distribution.py:69\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# skip checking lazily-constructed args\u001b[39;00m\n\u001b[0;32m     68\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[1;32m---> 69\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Spino.shop\\Desktop\\Trading\\Apps\\.MotherVenv\\lib\\site-packages\\torch\\distributions\\constraints.py:464\u001b[0m, in \u001b[0;36m_Simplex.check\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m ((value\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mabs() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "MAX_EPOCHS = 5000\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "N_TRIALS = 25\n",
    "REWARD_THRESHOLD = 200\n",
    "PRINT_INTERVAL = 10\n",
    "INPUT_DIM = env.observation_space.shape[0]\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = env.action_space.n\n",
    "DROPOUT = 0.5\n",
    "overallTimeStep = 0\n",
    "\n",
    "episodeReturns = []\n",
    "policy = policyNetwork(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT)\n",
    "optimizer = optim.Adam(policy.parameters(), lr = 0.01)\n",
    "\n",
    "_startTime = time.time()\n",
    "for episode in range(MAX_EPOCHS):\n",
    "    episodeReward, trajectoryRewards, log_probActions = forwardPass(env, policy, DISCOUNT_FACTOR)\n",
    "    updatePolicyNetwork(log_probActions, trajectoryRewards, optimizer)\n",
    "    \n",
    "    episodeReturns.append(episodeReward)\n",
    "    mean_episode_return = np.mean(episodeReturns[-N_TRIALS:])\n",
    "\n",
    "    if episode % PRINT_INTERVAL == 0:\n",
    "        print(f'| Episode: {episode:3} | Mean Rewards: {mean_episode_return:5.1f} | tps: {overallTimeStep/(time.time()-_startTime)}')\n",
    "\n",
    "    if mean_episode_return >= REWARD_THRESHOLD:\n",
    "        print(f'Reached reward threshold in {episode} episodes')\n",
    "        break\n",
    "    overallTimeStep += len(trajectoryRewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".MotherVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
