{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fb9c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b35238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.distributions import MultivariateNormal\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a974bcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Networks\n",
    "\n",
    "class feedForwardNN(nn.Module):\n",
    "    def __init__(self, inDim, outDim):\n",
    "        super(feedForwardNN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(inDim, 64)\n",
    "        self.layer2 = nn.Linear(64, 64)\n",
    "        self.layer3 = nn.Linear(64, outDim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x should be a tensor\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9c99ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, env):\n",
    "        # Define the hyper parameters\n",
    "        self._initHyperParameters()\n",
    "        \n",
    "        # Extract environment information\n",
    "        self.env = env\n",
    "        self.obsDim = env.observation_space.shape[0]\n",
    "        self.actDim = env.action_space.shape[0]\n",
    "        \n",
    "        # Define the networks\n",
    "        self.actor = feedForwardNN(self.obsDim, self.actDim)\n",
    "        self.critic = feedForwardNN(self.obsDim, 1)\n",
    "        \n",
    "        # Create our variable for the matrix.\n",
    "        # Note that I chose 0.5 for stdev arbitrarily.\n",
    "        self.cov_var = torch.full(size=(self.act_dim,), fill_value=0.5)\n",
    "        self.cov_mat = torch.diag(self.cov_var)\n",
    "    \n",
    "    def _initHyperParameters(self):\n",
    "        # The default hyper parameters of the PPO strategy\n",
    "        self.timeStepsPerBatch = 4800\n",
    "        self.maxTimeStepsPerEpisode = 1600\n",
    "        self.gamma = 0.95\n",
    "    \n",
    "    def getActions(self, obs):\n",
    "        mean = self.actor(obs)\n",
    "        dist = MultivariateNormal(mean, self.cov_mat)\n",
    "        action = dist.sample()\n",
    "        logProb = dist.log_prob(action)\n",
    "        \n",
    "        return action.detach().numpy(), logProb.detach()\n",
    "    \n",
    "    def rollout(self):\n",
    "        # The data collector\n",
    "        batchObs = []\n",
    "        batchActions = []\n",
    "        batchLogProbs = []\n",
    "        batchRewards = []\n",
    "        batchRewardsToGo = []\n",
    "        batchEpisodeLengths = []\n",
    "        \n",
    "        t = 0\n",
    "        while t < self.timeStepsPerBatch:\n",
    "            # Rewards per episode\n",
    "            episodeRewards = []\n",
    "            \n",
    "            obs = self.env.reset()\n",
    "            done = False\n",
    "            \n",
    "            for tEpisode in range(self.maxTimeStepsPerEpisode):\n",
    "                t += 1\n",
    "                \n",
    "                # Collect observations\n",
    "                batchObs.append(obs)\n",
    "                \n",
    "                action, logProb = self.getActions(obs)\n",
    "                obs, reward, done, info = self.env.step(action)\n",
    "                \n",
    "                episodeRewards.append(reward)\n",
    "                batchActions.append(action)\n",
    "                batchLogProbs.append(logProb)\n",
    "                \n",
    "                if done: break\n",
    "            \n",
    "            batchEpisodeLengths.append(tEpisode + 1)\n",
    "            batchRewards.append(episodeRewards)\n",
    "        \n",
    "        batchObs = torch.tensor(batchObs, dtype=torch.float32)\n",
    "        batchActions = torch.tensor(batchActions, dtype=torch.float32)\n",
    "        batchLogProbs = torch.tensor(batchLogProbs, dtype=torch.float32)\n",
    "        \n",
    "        batchRewardsToGo = self.computeRewardsToGo(batchRewards)\n",
    "        \n",
    "        return batchObs, batchActions, batchLogProbs, batchRewardsToGo, batchEpisodeLengths\n",
    "    \n",
    "    def computeRewardsToGo(self, batchRewards):\n",
    "        # The rewards-to-go per episode in each batch\n",
    "        batchRewardsToGo = []\n",
    "        \n",
    "        for episodeRewards in reversed(batchRewards):\n",
    "            discountedReward = 0\n",
    "            \n",
    "            for rew in reversed(episodeRewards):\n",
    "                discountedReward = rew + discountedReward * self.gamma\n",
    "                batchRewardsToGo.insert(0, discountedReward)\n",
    "        \n",
    "        batchRewardsToGo = torch.tensor(batchRewardsToGo, dtype=torch.float32)\n",
    "        \n",
    "        return batchRewardsToGo\n",
    "    \n",
    "    def learn(self, totalSteps):\n",
    "        t = 0\n",
    "        while t < totalSteps:            \n",
    "            batchObs, batchActions, batchLogProbs, batchRewardsToGo = self.rollout()\n",
    "            \n",
    "    def evaluate(self, batchObs):\n",
    "        # Query critic network for a value V for each obs in batchObs\n",
    "        V = self.critic(batchObs).squeeze()\n",
    "        \n",
    "        return V"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".MotherVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
