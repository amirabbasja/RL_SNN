{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN using gradient policy for training (Parallel training with multiple agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distributions\n",
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "import numpy as np\n",
    "import gymnasium as gym  \n",
    "import warnings\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the gymnasium environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "\n",
    "class qNetwork_SNN(nn.Module):\n",
    "    def __init__(self, inputSize, L1Size, L2Size, L3Size, L4Size, outputSize, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # Model super parameters\n",
    "        self.beta = kwargs[\"beta\"]\n",
    "        self.tSteps = kwargs[\"tSteps\"]\n",
    "\n",
    "        # Defining the layers\n",
    "        self.layer1 = nn.Linear(inputSize, L1Size)\n",
    "        self.L1LIF = snn.Leaky(beta = self.beta)\n",
    "        self.layer2 = nn.Linear(L1Size, L2Size)\n",
    "        self.L2LIF = snn.Leaky(beta = self.beta)\n",
    "        self.layer3 = nn.Linear(L2Size, L3Size)\n",
    "        self.L3LIF = snn.Leaky(beta = self.beta)\n",
    "        self.layer4 = nn.Linear(L3Size, L4Size)\n",
    "        self.L4LIF = snn.Leaky(beta = self.beta)\n",
    "        self.output = nn.Linear(L4Size, outputSize)\n",
    "        self.outputLIF = snn.Leaky(beta = self.beta)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Set initial potentials to be zero\n",
    "        potential1 = self.L1LIF.reset_mem()\n",
    "        potential2 = self.L2LIF.reset_mem()\n",
    "        potential3 = self.L3LIF.reset_mem()\n",
    "        potential4 = self.L4LIF.reset_mem()\n",
    "        potential5 = self.outputLIF.reset_mem()\n",
    "\n",
    "        # Save the state of the output layer\n",
    "        outSpikes = []\n",
    "        outPotentials = []\n",
    "\n",
    "        # Iterate through time steps\n",
    "        for t in range(self.tSteps):\n",
    "            # First layer\n",
    "            current1 = self.layer1(x)\n",
    "            spk1, potential1 = self.L1LIF(current1, potential1)\n",
    "\n",
    "            # Second layer\n",
    "            current2 = self.layer2(spk1)\n",
    "            spk2, potential2 = self.L2LIF(current2, potential2)\n",
    "\n",
    "            # Third layer\n",
    "            current3 = self.layer3(spk2)\n",
    "            spk3, potential3 = self.L3LIF(current3, potential3)\n",
    "\n",
    "            # Fourth layer\n",
    "            current4 = self.layer4(spk3)\n",
    "            spk4, potential4 = self.L4LIF(current4, potential4)\n",
    "\n",
    "            #Output\n",
    "            current5 = self.output(spk4)\n",
    "            spk5, potential5 = self.outputLIF(current5, potential5)\n",
    "\n",
    "            # Save output\n",
    "            outSpikes.append(spk5)\n",
    "            outPotentials.append(potential5)\n",
    "\n",
    "        return torch.stack(outSpikes, dim = 0).sum(dim = 0)\n",
    "\n",
    "\n",
    "# Define the network\n",
    "class policyNetwork(nn.Module):\n",
    "    def __init__(self, inputDim, hiddenDim, outDim, dropOut):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(inputDim, hiddenDim)\n",
    "        self.layer2 = nn.Linear(hiddenDim, outDim)\n",
    "        self.dropout = nn.Dropout(dropOut)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.dropout(x1)\n",
    "        x3 = F.relu(x2)\n",
    "        x4 = self.layer2(x3)\n",
    "        return x4\n",
    "\n",
    "# Calculate the cumulative returns in a specific trajectory\n",
    "def cumulativeReturn(trajectoryRewards, gamma):\n",
    "    # gamma is the discount factor\n",
    "    \n",
    "    returns = []\n",
    "    __R = 0\n",
    "    \n",
    "    for reward in reversed(trajectoryRewards):\n",
    "        __R = reward + gamma * __R\n",
    "        returns.insert(0, __R)\n",
    "    \n",
    "    returns = torch.tensor(returns)\n",
    "    \n",
    "    # Normalize the returns\n",
    "    return (returns - returns.mean())/(returns.std() + 1e-9)\n",
    "\n",
    "def cumulativeReturn_vectorized(rewards, gamma):\n",
    "    \"\"\"\n",
    "    Compute stepwise returns with a discount factor using a vectorized approach in PyTorch.\n",
    "    Explanation link: https://x.com/i/grok/share/1SAiyt3UzyHHDIE6h3ALxCyz2\n",
    "    \n",
    "    Parameters:\n",
    "    - rewards: 1D torch tensor of shape (T,) containing the reward sequence [r_0, r_1, ..., r_{T-1}]\n",
    "    - gamma: float, discount factor between 0 and 1\n",
    "    \n",
    "    Returns:\n",
    "    - returns: 1D torch tensor of shape (T,) containing [G_0, G_1, ..., G_{T-1}]\n",
    "    \"\"\"\n",
    "    # Ensure rewards is a 1D tensor\n",
    "    assert rewards.dim() == 1, \"rewards must be a 1D tensor\"\n",
    "    # print(\"input\", rewards)\n",
    "    T = rewards.size(0)\n",
    "    # Create indices for broadcasting\n",
    "    indices = torch.arange(T)\n",
    "    # Compute exponents (j - i) using broadcasting\n",
    "    exponents = indices[None, :] - indices[:, None]\n",
    "    # Construct the upper triangular matrix A\n",
    "    A = torch.where(exponents >= 0, gamma ** exponents.float(), torch.zeros_like(exponents).float())\n",
    "    # Compute returns via matrix multiplication\n",
    "    returns = A @ rewards\n",
    "    return (returns - returns.mean())/(returns.std() + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  60 | Mean Rewards:   9.5 | tps: 13.15\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 104>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    111\u001b[0m __loss \u001b[38;5;241m=\u001b[39m _loss \u001b[38;5;241m/\u001b[39m NUM_ENVS\n\u001b[0;32m    112\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 113\u001b[0m \u001b[43m__loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ep \u001b[38;5;129;01min\u001b[39;00m overallRewards: episodeReturns\u001b[38;5;241m.\u001b[39mappend(_ep)\n",
      "File \u001b[1;32mc:\\Users\\Spino.shop\\Desktop\\Trading\\Apps\\.MotherVenv\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Spino.shop\\Desktop\\Trading\\Apps\\.MotherVenv\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Spino.shop\\Desktop\\Trading\\Apps\\.MotherVenv\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[1;32mc:\\Users\\Spino.shop\\Desktop\\Trading\\Apps\\.MotherVenv\\lib\\site-packages\\torch\\autograd\\function.py:307\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m     )\n\u001b[0;32m    306\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[1;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Spino.shop\\Desktop\\Trading\\Apps\\.MotherVenv\\lib\\site-packages\\snntorch\\surrogate.py:200\u001b[0m, in \u001b[0;36mATan.backward\u001b[1;34m(ctx, grad_output)\u001b[0m\n\u001b[0;32m    195\u001b[0m (input_,) \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39msaved_tensors\n\u001b[0;32m    196\u001b[0m grad_input \u001b[38;5;241m=\u001b[39m grad_output\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m    197\u001b[0m grad \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    198\u001b[0m     ctx\u001b[38;5;241m.\u001b[39malpha\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m--> 200\u001b[0m     \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;241m*\u001b[39m grad_input\n\u001b[0;32m    202\u001b[0m )\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grad, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Synchronous environments (Network updates when all agents are done)\n",
    "# Works perfectly fine\n",
    "\n",
    "def forwardPass(envs, policyNetwork, discountFactor):\n",
    "    \"\"\"\n",
    "    Runs a forward pass through all environments and only after all of them are done\n",
    "    the pass finishes. After termination of each environment, the accumulation of \n",
    "    the environment's data stops; however, when envs.step(...) is ran, the terminated\n",
    "    environments are restarted and continued. This approach has been employed because \n",
    "    Gymnasium currently doesn't support running steps for particular environments, or\n",
    "    restarting particular environments.\n",
    "\n",
    "    Args:\n",
    "        envs (gymnasium vector environment): A gymnasium vector environment\n",
    "        policyNetwork (torch.nn): The network we are trying to learn\n",
    "        discountFactor (float): the discount factor to calculate rewards in a trajectory\n",
    "\n",
    "    Returns:\n",
    "        overallRewards: The accumulated reward of each agent's trajectory\n",
    "        trajectoryRewards: A list of pyTorch tensors\n",
    "        log_probActions: A list of pyTorch tensors\n",
    "    \"\"\"\n",
    "    log_probActions = [[] for _ in range(NUM_ENVS)]\n",
    "    rewards_hist = [[] for _ in range(NUM_ENVS)]\n",
    "    \n",
    "    activeEpisodes =  torch.ones(NUM_ENVS, dtype = torch.bool)\n",
    "    \n",
    "    states, _ = envs.reset(seed = random.randint(1,1_000_000_000))\n",
    "    \n",
    "    while True:\n",
    "        actionPred = policyNetwork(torch.Tensor(states[:,:4]))\n",
    "        actionProb = F.softmax(actionPred, dim = -1)\n",
    "        dist = distributions.Categorical(actionProb)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        states, rewards, terminated, truncated, _ = envs.step(action.numpy())\n",
    "\n",
    "        # Gather logarithm of actions and rewards for active envs (envs that are \n",
    "        # not terminated or truncated)\n",
    "        for idx in range(NUM_ENVS):\n",
    "            # Only add history if the episode has not been truncated\n",
    "            if activeEpisodes[idx]:\n",
    "                log_probActions[idx].append(dist.log_prob(action).unsqueeze(-1)[0])\n",
    "                rewards_hist[idx].append(torch.tensor(rewards[idx], dtype = torch.float).unsqueeze(-1))\n",
    "\n",
    "        # If an env is terminated or truncated, take it off the active env list\n",
    "        if ((terminated | truncated).any()):\n",
    "            _terminated = np.where((terminated | truncated) == True)[0]\n",
    "            for idx in _terminated: activeEpisodes[idx] = False\n",
    "        \n",
    "        # If all envs are terminated or truncated, stop the forward pass\n",
    "        if not activeEpisodes.any():\n",
    "            overallRewards = [torch.tensor(rewards_hist[idx], dtype = torch.float).sum().item() for idx in range(NUM_ENVS)]\n",
    "            trajectoryRewards = [cumulativeReturn_vectorized(torch.cat(rewards_hist[idx]), discountFactor) for idx in range(NUM_ENVS)]\n",
    "            log_probActions = [torch.cat(log_probActions[idx]) for idx in range(NUM_ENVS)]\n",
    "            break\n",
    "    \n",
    "    return overallRewards, trajectoryRewards, log_probActions\n",
    "\n",
    "\n",
    "NUM_ENVS = 1\n",
    "envs = gym.make_vec(\n",
    "    \"CartPole-v1\", \n",
    "    num_envs = NUM_ENVS, # Number of environments to create\n",
    "    vectorization_mode = \"async\",\n",
    "    wrappers = (gym.wrappers.TimeAwareObservation,),\n",
    ")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Async version with forward pass as a single function.\n",
    "# Hyperparameters\n",
    "MAX_EPOCHS = 5000\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "N_TRIALS = 25\n",
    "REWARD_THRESHOLD = 200\n",
    "PRINT_INTERVAL = 10\n",
    "INPUT_DIM = env.observation_space.shape[0]\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = env.action_space.n\n",
    "DROPOUT = 0.5\n",
    "\n",
    "episodeReturns = []\n",
    "# policy = policyNetwork(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT)\n",
    "nL1, nL2, nL3, nL4 = 256, 128, 64, 32\n",
    "learningRate = .0001\n",
    "timeSteps = 25\n",
    "snnBeta = .95\n",
    "eDecay = 0.999\n",
    "stateSize = env.observation_space.shape # Number of variables to define current step\n",
    "nActions = env.action_space.n # Number of actions\n",
    "policy = qNetwork_SNN(stateSize[0], nL1, nL2, nL3, nL4, nActions, beta = snnBeta, tSteps = timeSteps)\n",
    "optimizer = optim.Adam(policy.parameters(), lr = 0.01)\n",
    "\n",
    "\n",
    "mean_episode_return = 0\n",
    "episode = 0\n",
    "\n",
    "states, _ = envs.reset()\n",
    "overallTimeStep = 0\n",
    "# Setup the environment\n",
    "policy.train()\n",
    "_startTime = time.time()\n",
    "while True:\n",
    "    overallRewards, trajectoryRewards, log_probActions = forwardPass(envs, policy, DISCOUNT_FACTOR)\n",
    "    \n",
    "    _loss = 0\n",
    "    # Compute loss\n",
    "    for idx in range(NUM_ENVS):\n",
    "        _loss += -(log_probActions[idx] * trajectoryRewards[idx]).sum()\n",
    "    \n",
    "    __loss = _loss / NUM_ENVS\n",
    "    optimizer.zero_grad()\n",
    "    __loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    for _ep in overallRewards: episodeReturns.append(_ep)\n",
    "    mean_episode_return = np.mean(episodeReturns[-N_TRIALS:])\n",
    "\n",
    "    if episode % PRINT_INTERVAL == 0:\n",
    "        print(f'Episode: {episode:3} | Mean Rewards: {mean_episode_return:5.1f} | tps: {overallTimeStep/(time.time()-_startTime):.2f}', end=\"\\r\")\n",
    "\n",
    "    if mean_episode_return >= REWARD_THRESHOLD:\n",
    "        print(f'\\nReached reward threshold in {episode} forward passes')\n",
    "        break\n",
    "    \n",
    "    episode = episode + 1\n",
    "    overallTimeStep = overallTimeStep + max(map(len,trajectoryRewards))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".MotherVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
